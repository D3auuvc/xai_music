{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "We implemnt by `ballroom` [1], which has 8 genres & tempo & (down-)beats, 698 excerpts (30s)\n",
    "\n",
    "[1] [ballroom](http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dc/miniconda3/envs/env36/lib/python3.6/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempo list\n",
    "tmpo_fctr = {\"original\": 1.00,\n",
    "             \"x125\": 1.25,\n",
    "             \"x150\": 1.50,\n",
    "             \"x075\": 0.75,\n",
    "             \"x050\": 0.50}\n",
    "# Key\n",
    "keys = [-2, -1, 0, 1, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Stretch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stretch(signal, time_stretch_rate):\n",
    "    \"\"\"Time stretching implemented with librosa:\n",
    "    https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html?highlight=pitch%20shift#librosa.effects.pitch_shift\n",
    "    \"\"\"\n",
    "    return librosa.effects.time_stretch(signal, time_stretch_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch Scaleing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_scale(signal, sr, num_semitones):\n",
    "    \"\"\"Pitch scaling implemented with librosa:\n",
    "    https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html?highlight=pitch%20shift#librosa.effects.pitch_shift\n",
    "    \"\"\"\n",
    "    return librosa.effects.pitch_shift(signal, sr, num_semitones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_music_key(y, sr):\n",
    "    # pitches in 12 tone equal temperament\n",
    "    pitches = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    song_chroma = np.mean(librosa.feature.chroma_cqt(y=y, sr=sr), axis=1)\n",
    "    key = dict()\n",
    "\n",
    "    # select the most dominate pitch\n",
    "    pitch_id = np.argmax(song_chroma)\n",
    "    key['pitch'] = pitches[pitch_id]\n",
    "\n",
    "    min_third_id = (pitch_id+3) % 12\n",
    "    maj_third_id = (pitch_id+4) % 12\n",
    "\n",
    "    # check if the musical 3rd is major or minor\n",
    "    if song_chroma[min_third_id] < song_chroma[maj_third_id]:\n",
    "        key['third'] = 'Major'\n",
    "    elif song_chroma[min_third_id] > song_chroma[maj_third_id]:\n",
    "        key['third'] = 'Minor'\n",
    "\n",
    "    return key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_WAV_PATH = rf'audio/wav'\n",
    "_AUGMENT_PATH = rf'audio/augments'\n",
    "_WAV = '.wav'\n",
    "\n",
    "wav_audio_lst = Path(_WAV_PATH).glob('*.wav')\n",
    "\n",
    "for wa in wav_audio_lst:\n",
    "    # Load the audio as a waveform `y`\n",
    "    # Store the sampling rate as `sr`\n",
    "    y, sr = librosa.load(rf'{wa}')\n",
    "\n",
    "    # With diferent tempo\n",
    "    # scale (in this case stretch) the overall tempo by this factor\n",
    "    for tmp_n, tmp_v in tmpo_fctr.items():\n",
    "        augments_1_audio = time_stretch(y, tmp_v)\n",
    "\n",
    "        # With diferent key\n",
    "        for key in keys:\n",
    "            augments_2_audio = pitch_scale(augments_1_audio, sr, key)\n",
    "            key_analyze = get_music_key(augments_2_audio, sr)\n",
    "            key_note = ''\n",
    "            file_name = ''\n",
    "            if key == 0:\n",
    "                key_note = 'original'\n",
    "            else:\n",
    "                key_note = f\"{key_analyze['pitch']}{key_analyze['third']}\"\n",
    "\n",
    "            file_name = f\"{str(wa).split('/')[-1][:-4]}_{tmp_n}_{key_note}{_WAV}\"\n",
    "            sf.write(Path(_AUGMENT_PATH, file_name), augments_2_audio, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from musicnn_keras.tagger import top_tags\n",
    "\n",
    "\n",
    "musicnn = tf.keras.models.load_model(\n",
    "    './musicnn_keras/keras_checkpoints/MSD_musicnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing spectrogram (w/ librosa) and tags (w/ tensorflow).. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:07, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albums-Pais-Tropical-05', 'original', 'AMajor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b5c20533f118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0maudio_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'music'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0maudio_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tempo'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0maudio_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0maudio_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maudio_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "_MP3_PATH = rf'audio/augments'\n",
    "\n",
    "audios = Path(_MP3_PATH).glob('*.wav')\n",
    "audio_json = dict()\n",
    "\n",
    "for af in tqdm(audios):\n",
    "    # Get Tags\n",
    "    audio_dict = dict()\n",
    "    audio_dict['tags'] = top_tags(str(af),\n",
    "                                  model='MTT_musicnn',\n",
    "                                  topN=10,\n",
    "                                  print_tags=False)\n",
    "\n",
    "    # Save result\n",
    "    audio_name = str(af).split('/')[-1]\n",
    "    meta_data = audio_name.split('.')[0].split('_')\n",
    "    audio_dict['music'] = meta_data[0]\n",
    "    audio_dict['tempo'] = meta_data[1]\n",
    "    audio_dict['key'] = meta_data[2]\n",
    "    audio_json[audio_name.split('.')[0]] = audio_dict\n",
    "\n",
    "with open('result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(audio_json, f, ensure_ascii=False, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('env36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d666df2afcc0c6df9e28fa98bb526daf8825275d2a14a075fe129637dcee5a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
